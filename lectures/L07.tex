\documentclass[11pt]{article}
\usepackage{listings}
\usepackage{tikz}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{algorithm2e}
\usetikzlibrary{arrows,automata,shapes,positioning}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=2.5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{bw} = [rectangle, draw, fill=blue!20, 
    text width=3.5em, text centered, rounded corners, minimum height=2em]

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf ECE459: Programming for Performance } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\lstset{basicstyle=\ttfamily \scriptsize}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{#4}{Lecture #1}}
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

% http://gurmeet.net/2008/09/20/latex-tips-n-tricks-for-conference-papers/
\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }
\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }
\newcommand{\squishend}{
  \end{list}  }

\begin{document}

\lecture{7 --- January 19, 2015}{Winter 2015}{Patrick Lam}{version 0}

\paragraph{Conceptual view: non-blocking I/O.} Fundamentally,
there are two ways to find out whether I/O is ready to be queried:
polling (under UNIX, implemented via {\tt select}, {\tt poll},
and {\tt epoll}) and interrupts (under UNIX, signals).

We will describe {\tt epoll} in lecture. It is the most modern and
flexible interface. Unfortunately, I didn't realize that the obvious
{\tt curl} interface does not work with {\tt epoll} but instead with
{\tt select}.  There is different syntax but the ideas are the same.

The key idea is to give {\tt epoll} a bunch of file descriptors and
wait for events to happen. In particular:
     \begin{itemize}
       \item create an epoll instance ({\tt epoll\_create1});
       \item populate it with file descriptors ({\tt epoll\_ctl}); and
       \item wait for events ({\tt epoll\_wait}).
     \end{itemize}
Let's run through these steps in order.

\paragraph{Creating an {\tt epoll} instance.} Just use the API:
    \begin{lstlisting}
   int epfd = epoll_create1(0);
    \end{lstlisting}

The return value {\tt epfd} is typed like a UNIX file
descriptor---{\tt int}---but doesn't represent any files; instead, use
it as an identifier, to talk to {\tt epoll}.

The parameter ``{\tt 0}'' represents the flags, but the only available flag
is {\tt EPOLL\_CLOEXEC}. Not interesting to you.

\paragraph{Populating the {\tt epoll} instance.} Next, you'll want
{\tt epfd} to do something. The obvious thing is to add some {\tt fd}
to the set of descriptors watched by {\tt epfd}:
    \begin{lstlisting}
   struct epoll_event event;
   int ret;
   event.data.fd = fd;
   event.events = EPOLLIN | EPOLLOUT;
   ret = epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &event);
    \end{lstlisting}

You can also use {\tt epoll\_ctl} to modify and delete descriptors from {\tt epfd}; read the manpage to find out how.

\paragraph{Waiting on an {\tt epoll} instance.} Having completed
the setup, we're ready to wait for events on any file descriptor in {\tt epfd}.
    \begin{lstlisting}
  #define MAX_EVENTS 64

  struct epoll_event events[MAX_EVENTS];
  int nr_events;

  nr_events = epoll_wait(epfd, events, MAX_EVENTS, -1);
    \end{lstlisting}

The given {\tt -1} parameter means to wait potentially forever;
otherwise, the parameter indicates the number of milliseconds to wait.
(It is therefore ``easy'' to sleep for some number of milliseconds by
starting an {\tt epfd} and using {\tt epoll\_wait}; takes two function
calls instead of one, but allows sub-second latency.)

Upon return from {\tt epoll\_wait}, we know that we have {\tt
  nr\_events} events ready.

\subsection*{Level-Triggered and Edge-Triggered Events}
One relevant concept for these polling APIs is the concept of
\emph{level-triggered} versus \emph{edge-triggered}.  The default {\tt
  epoll} behavious is level-triggered: it returns whenever data is
ready. One can also specify (via {\tt epoll\_ctl}) edge-triggered
behaviour: return whenever there is a change in readiness.

We'll have a live coding demo in lecture 7.

\subsection*{Asynchronous I/O}
As mentioned above, the POSIX standard defines {\tt aio} calls.
Unlike just giving the {\tt O\_NONBLOCK} flag, using {\tt aio} works
for disk as well as sockets.

\paragraph{Key idea.} You specify the action to occur when I/O is ready:
    \begin{itemize}
      \item nothing;
      \item start a new thread; or
      \item raise a signal.
    \end{itemize}

Your code submits the requests using e.g. {\tt aio\_read} and {\tt aio\_write}.
If needed, wait for I/O to happen using {\tt aio\_suspend}.

\paragraph{Nonblocking I/O with curl.} The next lecture notes give more clue
about nonblocking I/O with curl. Although it doesn't work with {\tt epoll}
but rather {\tt select}, it uses the same ideas---we'll therefore see two
(three, with aio) different implementations of the same idea. 
Briefly, you:
\begin{itemize}
\item build up a set of descriptors;
\item invoke the transfers and wait for them to finish; and
\item see how things went.
\end{itemize}

\paragraph{Edge-triggered vs level-triggered.} We did a live coding demo
and I promised more details in the notes. The example was some code
(see {\tt socket.c} in the code examples) that created a server and
read from that server in either level-triggered mode or edge-triggered
mode. 

One would think that level-triggered mode would return from {\tt read}
whenever data was available, while edge-triggered mode would return
from {\tt read} whenever new data came in. Level-triggered does behave
as one would guess: if there is data available, {\tt read()} returns
the data. However, edge-triggered mode returns whenever the
state-of-readiness of the socket changes (from no-data-available to
data-available). Play with it and get a sense for how it works.

Good question to think about: when is it appropriate to choose one or the other?

\section*{curl\_multi}
It's important to see at least one specific example of an idea. I talked about
{\tt epoll} last time and I meant that to be the specific example, but we 
can't quite use it without getting into socket programming, and I don't want to
do that. Instead, we'll see non-blocking I/O in the specific example of the curl
library, which is reasonably widely used in the Linux world.

Tragically, it's complicated to use {\tt epoll} with {\tt curl\_multi}, and I couldn't
quite figure it out. So I'll describe the {\tt select}-based interface for {\tt curl\_multi}.
A socket-based interface which works with {\tt epoll} also exists. I won't talk about that.

The relevant steps, in any case, are:
\squishlist
\item Create individual requests with {\tt curl\_easy\_init}.
\item Create a multi-handle with {\tt curl\_multi\_init} and add the requests to it
with {\tt curl\_multi\_add\_handle}.
\item (for select-based interface:) put in requests \& wait for results, using {\tt 
curl\_multi\_perform}. That call generalizes {\tt curl\_easy\_perform}.
\item Handle completed transfers with {\tt curl\_multi\_info\_read}.
\squishend

\paragraph{On the use of {\tt curl\_multi\_perform}.} The actual non-blocking read/write
is done in {\tt curl\_multi\_perform}, which returns the number of still-active handles 
through its parameter.

You call it in a loop, with a call to {\tt select} above. Call {\tt select} and then
{\tt curl\_multi\_perform} in a loop while there are still running transfers.
You're also allowed to manipulate (delete/alter/re-add) a {\tt curl\_easy\_handle} whenever
a transfer finishes.

\paragraph{Setting up the {\tt select}.} Before you call {\tt curl\_multi\_perform}
and {\tt select}, you need to set up the {\tt select}. The curl call {\tt curl\_multi\_fdset}
sets up the parameters for the {\tt select}, while {\tt curl\_multi\_timeout} gives you
the proper timeout to hand to {\tt select}.

\begin{lstlisting}
    // zero the fd-sets
    FD_ZERO(&fdread); FD_ZERO(&fdwrite); FD_ZERO(&fdexcep);
    // retrieve the fds, check for error
    curl_multi_fdset(multi_handle, 
                     &fdread, &fdwrite, &fdexcep, &maxfd);
    if (maxfd < -1) abort_("multi_fdset: couldn't wait for fds");
    // retrieve the timeout
    curl_multi_timeout(multi_handle, &curl_timeout);
\end{lstlisting}

In an API infelicity, you have to convert the {\tt curl\_timeout} into a
{\tt struct timeval} for use by {\tt select}.

\paragraph{Calling {\tt select}.}
The call itself is fairly straightforward:
\begin{lstlisting}
  rc = select(maxfd + 1, &fdread, &fdwrite, &fdexcep, &timeout);
  if (rc == -1) abort_("[main] select error");
\end{lstlisting}
This waits for one of the file descriptors to become ready, or for the
timeout to elapse (whichever happens first).

Of course, once {\tt select} returns, you only know that something
happened, but you haven't done the work yet. So you then need to call
{\tt curl\_multi\_perform} again to do the work.

Finally, you get the results of {\tt curl\_multi\_perform} by calling
{\tt curl\_multi\_info\_read}. It also tells you how many messages are left.
\begin{lstlisting}
  msg = curl_multi_info_read(multi_handle, &msgs_left);
\end{lstlisting}
The return value \verb+msg->msg+ can be either {\tt CURLMSG\_DONE} or an error.
The handle \verb+msg->easy_handle+ tells you which handle finished. You may have
to look that up in your collection of handles.

\paragraph{Cleanup.} Always clean up after yourself! Use {\tt curl\_multi\_cleanup}
to destroy the multi-handle and {\tt curl\_easy\_cleanup} to destroy each individual handle.

\paragraph{Example.}
There is a not-great example at 
\begin{center}
\url{http://curl.haxx.se/libcurl/c/multi-app.html}
\end{center}
but I'm not even sure it works verbatim. You could use it as a solution template,
but you'll need to add more code---I asked you to replace completed transfers in the
{\tt curl\_multi}.

\paragraph{About that socket-based alternative.} There is yet another interface which
would allow you to use {\tt epoll}, but I couldn't figure it out. Sorry. The advantage,
beyond using {\tt epoll}, is that {\tt libcurl} doesn't need to scan over all of the transfers
when it receives notice that a transfer is ready. This can help when there are lots of sockets
open.

From the manpage:
\squishlist
\item Create a multi handle

\item Set the socket callback with {\tt CURLMOPT\_SOCKETFUNCTION}

\item Set the timeout callback with {\tt CURLMOPT\_TIMERFUNCTION}, to get to know what timeout value to use when waiting for socket activities.

\item Add easy handles with {\tt curl\_multi\_add\_handle()}

\item Provide some means to manage the sockets libcurl is using, so you can check them for activity. This can be done through your application code, or by way of an external library such as libevent or glib.

\item Call {\tt curl\_multi\_socket\_action(..., CURL\_SOCKET\_TIMEOUT, 0, ...)} to kickstart everything. To get one or more callbacks called.

\item Wait for activity on any of libcurl's sockets, use the timeout value your callback has been told.

\item When activity is detected, call {\tt curl\_multi\_socket\_action()} for the socket(s) that got action. If no activity is detected and the timeout expires, call {\tt curl\_multi\_socket\_action(3)} with {\tt CURL\_SOCKET\_TIMEOUT}. 
\squishend
There's an example, which has too many moving parts, here:
\begin{center}
\url{http://curl.haxx.se/libcurl/c/hiperfifo.html}
\end{center}
It uses {\tt libevent}, which I totally don't want to talk about in this class.

\section*{Race Conditions}
Previous courses should have introduced the concept of a race condition.
We'll be talking about them in greater detail in this course.

\paragraph{Definition.} A race occurs when you have two concurrent accesses to the
same memory location, at least one of which is a {\bf write}.

When there's a race, the final state may not be the same as running
one access to completion and then the other. (But it sometimes is.)
Race conditions typically arise between variables which are shared
between threads.

\paragraph{Example.}
\begin{lstlisting}
#include <stdlib.h>
#include <stdio.h>
#include <pthread.h>

void* run1(void* arg)
{
    int* x = (int*) arg;
    *x += 1;
}

void* run2(void* arg)
{
    int* x = (int*) arg;
    *x += 2;
}

int main(int argc, char *argv[])
{
    int* x = malloc(sizeof(int));
    *x = 1;
    pthread_t t1, t2;
    pthread_create(&t1, NULL, &run1, x);
    pthread_join(t1, NULL);
    pthread_create(&t2, NULL, &run2, x);
    pthread_join(t2, NULL);
    printf("%d\n", *x);
    free(x);
    return EXIT_SUCCESS;
}
\end{lstlisting}

\noindent
Question: Do we have a data race? Why or why not?
\vspace*{2em}
%No, we don't. Only one thread is active at a time.

\paragraph{Example 2.} Here's another example; keep the same thread definitions.
\begin{lstlisting}
int main(int argc, char *argv[])
{
    int* x = malloc(sizeof(int));
    *x = 1;
    pthread_t t1, t2;
    pthread_create(&t1, NULL, &run1, x);
    pthread_create(&t2, NULL, &run2, x);
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    printf("%d\n", *x);
    free(x);
    return EXIT_SUCCESS;
}
\end{lstlisting}

Now do we have a data race? Why or why not?
\vspace*{2em}

% Yes, we do. We have 2 threads concurrently accessing the same data.

\paragraph{Tracing our Example Data Race.} 
What are the possible outputs? (Assume that initially {\tt *x} is 1.)
We'll look at compiler intermediate code (three-address code) to tell.

\hspace*{.2\textwidth}\begin{minipage}{.8\textwidth}
\begin{lstlisting}[numbers=left]
run1                          run2   
D.1 = *x;                     D.1 = *x;
D.2 = D.1 + 1;                D.2 = D.1 + 2
*x = D.2;                     *x = D.2;
  \end{lstlisting}
\end{minipage}

Memory reads and writes are key in data~races.

Let's call the read and write from {\tt run1} R1 and W1; R2 and W2
from {\tt run2}. Assuming a sane\footnote{sequentially consistent; sadly, many
widely-used models are wilder than this.}
memory model, $R_n$ must precede $W_n$.

\newpage
Here are all possible orderings:
  \begin{center}
    \begin{tabular}{llll|l}
\multicolumn{4}{c|}{Order} & {\tt *x}\\
\hline
R1 & W1 & R2 & W2 & 4 \\
R1 & R2 & W1 & W2 & 3 \\
R1 & R2 & W2 & W1 & 2 \\
R2 & W2 & R1 & W1 & 4 \\
R2 & R1 & W2 & W1 & 2 \\
R2 & R1 & W1 & W2 & 3 \\
    \end{tabular}
  \end{center}

\subsection*{Detecting Data Races Automatically}  
Dynamic and static tools exist. They can help you find data races in
your program. {\tt helgrind} is one such tool. It runs your program 
and analyzes it (and causes a large slowdown).

Run with {\tt valgrind --tool=helgrind <prog>}.

It will warn you of possible data races along with locations. For
useful debugging information, compile your program with debugging
information ({\tt -g} flag for {\tt gcc}).

\paragraph{Helgrind Output for Example.}
\begin{lstlisting}
==5036== Possible data race during read of size 4 at
         0x53F2040 by thread #3
==5036== Locks held: none
==5036==    at 0x400710: run2 (in datarace.c:14)
...
==5036== 
==5036== This conflicts with a previous write of size 4 by
         thread #2
==5036== Locks held: none
==5036==    at 0x400700: run1 (in datarace.c:8)
...
==5036== 
==5036== Address 0x53F2040 is 0 bytes inside a block of size
         4 alloc'd
...
==5036==    by 0x4005AE: main (in datarace.c:19)
\end{lstlisting}

\section*{Synchronization}
You'll need some sort of synchronization to get sane results from
multithreaded programs. We'll start by talking about how to use
mutual exclusion in Pthreads.

\paragraph{Mutual Exclusion.} Mutexes are the most basic type of synchronization.
As a reminder:
    \begin{itemize}
    \item Only one thread can access code protected by a mutex at a time.
    \item All other threads must wait until the mutex is free before they can
      execute the protected code.
    \end{itemize}

Here's an example of using mutexes:
  \begin{lstlisting}
pthread_mutex_t m1_static = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t m2_dynamic;

pthread_mutex_init(&m2_dynamic, NULL);
...
pthread_mutex_destroy(&m1_static);
pthread_mutex_destroy(&m2_dynamic);
  \end{lstlisting}

You can initialize mutexes statically (as with {\tt m1\_static}) or
dynamically ({\tt m2\_dynamic}). If you want to include attributes,
you need to use the dynamic version.

\paragraph{Mutex Attributes.} Both threads and mutexes use the notion of attributes.
We won't talk about mutex attributes in any detail, but here are the three standard ones.
  \begin{itemize}
    \item {\bf Protocol}: specifies the protocol used to prevent priority
      inversions for a mutex.
    \item {\bf Prioceiling}: specifies the priority ceiling of a mutex.
    \item {\bf Process-shared}: specifies the process sharing of a mutex.
  \end{itemize}
  You can specify a mutex as {\it process shared} so that you can access it
  between processes. In that case, you need to use shared memory and {\tt mmap},
  which we won't get into.

\paragraph{Mutex Example.} Let's see how this looks in practice. It is fairly simple:
  \begin{lstlisting}
// code
pthread_mutex_lock(&m1);
// protected code
pthread_mutex_unlock(&m1);
// more code
  \end{lstlisting}

  \begin{itemize}
    \item Everything within the {\tt lock} and {\tt unlock} is protected.
    \item Be careful to avoid deadlocks if you are using multiple mutexes (always
acquire locks in the same order across threads).
    \item Another useful primitive is {\tt pthread\_mutex\_trylock}. We may come back to this
later.
  \end{itemize}

\subsection*{Data Races}
Why are we bothering with locks? Data races. A data race occurs when
two concurrent actions access the same variable and at least one of
them is a {\bf write}. (This shows up on Assignment 1!)

  \begin{lstlisting}
...
static int counter = 0;

void* run(void* arg) {
    for (int i = 0; i < 100; ++i) {
        ++counter;
    }
}

int main(int argc, char *argv[])
{
    // Create 8 threads
    // Join 8 threads
    printf("counter = %i\n", counter);
}
  \end{lstlisting}

Is there a datarace in this example? If so, how would we fix it?

\paragraph{Solution: use mutexes.}
  \begin{lstlisting}[escapechar=!]
...
!\bf{static pthread\_mutex\_t mutex = PTHREAD\_MUTEX\_INITIALIZER;}!
static int counter = 0;

void* run(void* arg) {
    for (int i = 0; i < 100; ++i) {
        !\bf{pthread\_mutex\_lock(\&mutex);}!
        ++counter;
        !\bf{pthread\_mutex\_unlock(\&mutex);}!
    }
}

int main(int argc, char *argv[])
{
    // Create 8 threads
    // Join 8 threads
    !\bf{pthread\_mutex\_destroy(\&mutex);}!
    printf("counter = %i\n", counter);
}
  \end{lstlisting}

\section*{More Synchronization primitives}

We'll proceed in order of complexity.

\paragraph{Recap: mutexes.} Recall that our goal in this course is
to be able to use mutexes correctly. You should have seen how to 
implement them in an operating systems course. Here's how to use them.

\begin{itemize}
\item Call {\tt lock} on mutex $\ell_1$. Upon return from
      {\tt lock}, your thread has exclusive access to $\ell_1$ until it
      {\tt unlock}s it.
\item Other calls to {\tt lock} $\ell_1$ will not return
      until {\tt m1} is available.
\end{itemize}

 For background on implementing mutual exclusion, see
 \href{http://en.wikipedia.org/wiki/Lamport\%27s_bakery_algorithm}
      {Lamport's bakery algorithm}. Implementation details are not in
      scope for this course.

Key idea: locks protect resources; only one thread
can hold a lock at a time. A second thread trying to obtain the lock
(i.e. \emph{contending} for the lock) has to wait, or \emph{block},
until the first thread releases the lock. So only one thread has
access to the protected resource at a time. The code between the lock
acquisition and release is known as the \emph{critical region}.

Some mutex implementations also provide a ``try-lock'' primitive,
which grabs the lock if it's available, or returns control to the
thread if it's not, thus enabling the thread to do something else. (Kind of
like non-blocking I/O!)

Excessive use of locks can serialize programs. Consider two resources
$A$ and $B$ protected by a single lock $\ell$. Then a thread that's
just interested in $B$ still has acquire $\ell$, which requires it to
wait for any other thread working with $A$. (The Linux kernel used to
rely on a Big Kernel Lock protecting lots of resources in the 2.0 era,
and Linux 2.2 improved performance on SMPs by cutting down on the use
of the BKL.)

Note: in Windows, the term ``mutex'' refers to an inter-process
communication mechanism. ``Critical sections'' are the mutexes we're
talking about above.

\paragraph{Spinlocks.} Spinlocks are a variant of mutexes, where the
waiting thread repeatedly tries to acquire the lock instead of sleeping.
Use spinlocks when you expect critical sections to finish 
quickly\footnote{For more information on spinlocks in the Linux
kernel, see \url{http://lkml.org/lkml/2003/6/14/146}.}. Spinning
for a long time consumes lots of CPU resources. Many lock
implementations use both sleeping and spinlocks: spin for a bit,
then sleep for longer. At some point, we saw a live coding example
comparing spinlocks to normal mutexes.

\paragraph{Reader/Writer Locks.} Recall that data races only happen when
one of the concurrent accesses is a write. So, if you have read-only
(``immutable'') data, as often occurs in functional programs, you don't need
to protect access to that data. For instance, your program might
have an initialization phase, where you write some data, and then a 
query phase, where you use multiple threads to read the data.

Unfortunately, sometimes your data is not read-only. It might, for instance,
be rarely updated. Locking the data every time would be inefficient.
The answer is to instead use a \emph{reader/writer} lock. Multiple
threads can hold the lock in read mode, but only one thread can hold the
lock in write mode, and it will block until all the readers are done 
reading.

\begin{verbatim}
  int readData(int c1, int c2) {              // glib usage example
    g_static_rw_lock_reader_lock (&rwlock);
    int result = data[c1] + data[c2];
    g_static_rw_lock_reader_unlock (&rwlock);
  }

  void writeData(int c1, int c2, int value) {
    g_static_rw_lock_writer_lock (&rwlock);
    data[c1] += value; data[c2] -= value;
    g_static_rw_lock_writer_unlock (&rwlock);
  }
\end{verbatim}

\paragraph{Semaphores/condition variables.} 
While semaphores can keep track of a counter and can implement
mutexes, you should use them to support signalling between threads or
processes.

In pthreads, semaphores can also be used for inter-process communication,
while condition variables are like Java's {\tt wait()}/{\tt notify()}.

\paragraph{Barriers.} This synchronization primitive allows you 
to make sure that a collection of threads all reach the
barrier before finishing. In pthreads, each thread should call
\verb+pthread_barrier_wait()+, which will proceed when enough threads
have reached the barrier. Enough means a number you specify upon
barrier creation.

\paragraph{Lock-Free Code.} We'll talk more about this in a few weeks.
Modern CPUs support atomic operations, such as compare-and-swap, which
enable experts to write lock-free code. A recent research 
result~\cite{mckenney11:_concur,Attiya:2011:LOE:1926385.1926442} states the requirements for correct implementations: basically,
such implementations must contain certain synchronization constructs.

\section*{Semaphores}
As you learned in previous courses, semaphores have a {\tt value} and
can be used for signalling between threads. When you create a semaphore,
you specify an initial value for that semaphore. Here's how they work.

\begin{itemize}
\item The {\tt value} can be understood to represent the number of resources available.
\item A semaphore has two fundamental operations: {\tt wait} and 
{\tt post}.
\item {\tt wait} reserves one instance of the protected resource, if currently
available---that is, if {\tt value} is currently above 0. If {\tt value} 
is 0, then {\tt wait} suspends the thread until some other thread makes
the resource available.
\item {\tt post} releases one instance of the protected resource,
incrementing {\tt value}.
\end{itemize}

\paragraph{Semaphore Usage.} Here are the relevant API calls.
  \begin{lstlisting}
#include <semaphore.h>

int sem_init(sem_t *sem, int pshared, unsigned int value);
int sem_destroy(sem_t *sem);
int sem_post(sem_t *sem);
int sem_wait(sem_t *sem);
int sem_trywait(sem_t *sem);
  \end{lstlisting}

This API is a lot like the mutex API:
  \begin{itemize}
    \item must link with {\tt -pthread} (or {\tt -lrt} on Solaris);
    \item all functions return {\tt 0} on success;
    \item same usage as mutexes in terms of passing pointers.
  \end{itemize}

\noindent
How could you use a {\tt semaphore} as a {\tt mutex}?
\vspace*{3em}

% If the initial {\tt value} is 1 and you use {\tt wait} to lock
% and {\tt post} to unlock, it's equivalent to a {\tt mutex}

\paragraph{Semaphores for Signalling.}
Here's an example from the book. How would you make this always print
``Thread 1'' then ``Thread 2'' using semaphores?

\begin{lstlisting}
#include <pthread.h>
#include <stdio.h>
#include <semaphore.h>
#include <stdlib.h>

void* p1 (void* arg) { printf("Thread 1\n"); }

void* p2 (void* arg) { printf("Thread 2\n"); }

int main(int argc, char *argv[])
{
    pthread_t thread[2];
    pthread_create(&thread[0], NULL, p1, NULL);
    pthread_create(&thread[1], NULL, p2, NULL);
    pthread_join(thread[0], NULL);
    pthread_join(thread[1], NULL);
    return EXIT_SUCCESS;
}
\end{lstlisting}

\paragraph{Proposed Solution.} Is it actually correct?

\begin{lstlisting}
sem_t sem;
void* p1 (void* arg) {
  printf("Thread 1\n");
  sem_post(&sem);
}
void* p2 (void* arg) {
  sem_wait(&sem);
  printf("Thread 2\n");
}

int main(int argc, char *argv[])
{
    pthread_t thread[2];
    sem_init(&sem, 0, /* value: */ 1);
    pthread_create(&thread[0], NULL, p1, NULL);
    pthread_create(&thread[1], NULL, p2, NULL);
    pthread_join(thread[0], NULL);
    pthread_join(thread[1], NULL);
    sem_destroy(&sem);
}
\end{lstlisting}

Well, let's reason through it.  
  \begin{itemize}
    \item {\tt value} is initially 1.
    \item Say {\tt p2} hits its {\tt sem\_wait} first and succeeds.
    \item {\tt value} is now 0 and {\tt p2} prints ``Thread 2'' first.
    \item It would be OK if {\tt p1} happened first. That would just increase
      {\tt value} to 2.
    \end{itemize}

Fix: set the initial {\tt value} to 0. Then, if {\tt p2} hits
its {\tt sem\_wait} first, it will not print until {\tt p1} posts, which is after 
{\tt p1} prints ``Thread 1''.


\section*{Race Conditions}
We'll next use our knowledge of three address code to analyze
potential race conditions more rigourously.

\paragraph{Definition.} A race occurs when you have two concurrent accesses to the
same memory location, at least one of which is a {\bf write}.

When there's a race, the final state may not be the same as running
one access to completion and then the other. (But it sometimes is.)
Race conditions typically arise between variables which are shared
between threads.

\paragraph{Example.}
\begin{lstlisting}
#include <stdlib.h>
#include <stdio.h>
#include <pthread.h>

void* run1(void* arg)
{
    int* x = (int*) arg;
    *x += 1;
}

void* run2(void* arg)
{
    int* x = (int*) arg;
    *x += 2;
}

int main(int argc, char *argv[])
{
    int* x = malloc(sizeof(int));
    *x = 1;
    pthread_t t1, t2;
    pthread_create(&t1, NULL, &run1, x);
    pthread_join(t1, NULL);
    pthread_create(&t2, NULL, &run2, x);
    pthread_join(t2, NULL);
    printf("%d\n", *x);
    free(x);
    return EXIT_SUCCESS;
}
\end{lstlisting}

\noindent
Question: Do we have a data race? Why or why not?
\vspace*{2em}
%No, we don't. Only one thread is active at a time.

\paragraph{Example 2.} Here's another example; keep the same thread definitions.
\begin{lstlisting}
int main(int argc, char *argv[])
{
    int* x = malloc(sizeof(int));
    *x = 1;
    pthread_t t1, t2;
    pthread_create(&t1, NULL, &run1, x);
    pthread_create(&t2, NULL, &run2, x);
    pthread_join(t1, NULL);
    pthread_join(t2, NULL);
    printf("%d\n", *x);
    free(x);
    return EXIT_SUCCESS;
}
\end{lstlisting}

Now do we have a data race? Why or why not?
\vspace*{2em}

% Yes, we do. We have 2 threads concurrently accessing the same data.

\paragraph{Tracing our Example Data Race.} 
What are the possible outputs? (Assume that initially {\tt *x} is 1.)
We'll look at the three-address code to tell.

\hspace*{.2\textwidth}\begin{minipage}{.8\textwidth}
\begin{lstlisting}[numbers=left]
run1                          run2   
D.1 = *x;                     D.1 = *x;
D.2 = D.1 + 1;                D.2 = D.1 + 2
*x = D.2;                     *x = D.2;
  \end{lstlisting}
\end{minipage}

Memory reads and writes are key in data~races.

Let's call the read and write from {\tt run1} R1 and W1; R2 and W2
from {\tt run2}. Assuming a sane\footnote{sequentially consistent; sadly, many
widely-used models are wilder than this.}
memory model, $R_n$ must precede $W_n$.

\newpage
Here are all possible orderings:
  \begin{center}
    \begin{tabular}{llll|l}
\multicolumn{4}{c|}{Order} & {\tt *x}\\
\hline
R1 & W1 & R2 & W2 & 4 \\
R1 & R2 & W1 & W2 & 3 \\
R1 & R2 & W2 & W1 & 2 \\
R2 & W2 & R1 & W1 & 4 \\
R2 & R1 & W2 & W1 & 2 \\
R2 & R1 & W1 & W2 & 3 \\
    \end{tabular}
  \end{center}

\subsection*{Detecting Data Races Automatically}  
Dynamic and static tools exist. They can help you find data races in
your program. {\tt helgrind} is one such tool. It runs your program 
and analyzes it (and causes a large slowdown).

Run with {\tt valgrind --tool=helgrind <prog>}.

It will warn you of possible data races along with locations. For
useful debugging information, compile your program with debugging
information ({\tt -g} flag for {\tt gcc}).

\paragraph{Helgrind Output for Example.}
\begin{lstlisting}
==5036== Possible data race during read of size 4 at
         0x53F2040 by thread #3
==5036== Locks held: none
==5036==    at 0x400710: run2 (in datarace.c:14)
...
==5036== 
==5036== This conflicts with a previous write of size 4 by
         thread #2
==5036== Locks held: none
==5036==    at 0x400700: run1 (in datarace.c:8)
...
==5036== 
==5036== Address 0x53F2040 is 0 bytes inside a block of size
         4 alloc'd
...
==5036==    by 0x4005AE: main (in datarace.c:19)
\end{lstlisting}


\bibliographystyle{alpha}
\bibliography{L07}


\end{document}
