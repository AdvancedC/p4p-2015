\documentclass[aspectratio=43]{beamer}

% Text packages to stop warnings
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{multirow}
\usepackage{tikz}

\usetikzlibrary{arrows,automata,shapes,positioning}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=2.5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{bw} = [rectangle, draw, fill=blue!20, 
    text width=3.5em, text centered, rounded corners, minimum height=2em]

% Themes
\usetheme{Boadilla}
\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{navigation symbols}{}

% Suppress the navigation bar
\beamertemplatenavigationsymbolsempty

\lstset{basicstyle=\scriptsize, frame=single}

\newenvironment{changemargin}[1]{% 
  \begin{list}{}{% 
    \setlength{\topsep}{0pt}% 
    \setlength{\leftmargin}{#1}% 
    \setlength{\rightmargin}{1em}
    \setlength{\listparindent}{\parindent}% 
    \setlength{\itemindent}{\parindent}% 
    \setlength{\parsep}{\parskip}% 
  }% 
  \item[]}{\end{list}} 

\title{Lecture 16---Automatic Parallelization}
\subtitle{ECE 459: Programming for Performance}
\date{February 9, 2015}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain]
  \titlepage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Road Map}

  \begin{changemargin}{1cm}
    \begin{itemize}
    \item Past: manual parallelization with threads
    \item Now: compilers \& automatic parallelization (when)
    \item Future: under the hood (how compilers parallelize))
    \end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Example Code to Parallelize}

\begin{changemargin}{1.5cm}
  \begin{lstlisting}[numbers=left]
#include <stdlib.h>

void setup(double *vector, int length) {
    int i;
    for (i = 0; i < length; i++)
    {
        vector[i] += 1.0;
    }
}

int main()
{
    double *vector;
    vector = (double*) malloc(sizeof(double)*1024*1024);
    for (int i = 0; i < 1000; i++)
    {
        setup (vector, 1024*1024);
    }
}
  \end{lstlisting}
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Automatic Parallelization of Example Code}

\begin{changemargin}{1.5cm}
  Let's try automatic parallelization.
  \vfill
  Compiling with {\tt solarisstudio} and automatic parallelization yields
  the following:
\end{changemargin}

{\scriptsize
  \begin{lstlisting}
% solarisstudio-cc -O3 -xautopar -xloopinfo omp_vector.c 
"omp_vector.c", line 5: PARALLELIZED, and serial version generated                 
"omp_vector.c", line 15: not parallelized, call may be unsafe
  \end{lstlisting}
}
\begin{changemargin}{1.5cm}
  How will this code compare to our manual efforts? \\
  (If you weren't in class, you'll have to try it yourself.)
  \vfill
  {\bf Note:} {\tt solarisstudio} generates two versions of the code, 
  and decides, at runtime, if the parallel code would be faster.
\end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Autoparallelization implementation: OpenMP}

\begin{changemargin}{1.5cm}
  Under the hood, most parallelization frameworks use {\tt OpenMP},
      which we'll see next lecture.\\[1em]
  For now: you can control the number of threads with the
      {\tt OMP\_NUM\_THREADS} environment variable.\\[1em]
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Automatic Parallelization in {\tt gcc}}

\begin{changemargin}{1.5cm}
  {\tt gcc} (since 4.3) can also auto-parallelize loops. \\
  However, there are
      a few problems:
      \begin{enumerate}
        \item It will not tell you which loops it parallelizes (nicely).
        \item It only operates with a fixed number of threads.
        \item The profitability metrics are quite simple.
        \item Only operates in simple cases.
      \end{enumerate}
   ~\\[1em]

  Use the flags {\tt -floop-parallelize-all -ftree-parallelize-loops=N} where {\tt N} is the \# of
  threads.
  \vfill
  {\bf Note:} {\tt gcc} also uses {\tt OpenMP} but ignores
  {\tt OMP\_NUM\_THREADS}.
\end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Understanding Automatic Parallelization in {\tt gcc}}

\begin{changemargin}{1.5cm}
  Flag {\tt -fdump-tree-parloops-details} shows what the automatic
  parallelizations were, but it's quite unreadable.
  \vfill
  Instead, you can look at the assembly code to see the parallelizations
  (obviously, impractical for a large project).
  \begin{lstlisting}
% gcc -std=c99 -O3 -ftree-parallelize-loops=4
  omp_vector_gcc.c -S -o omp_vector_gcc_auto.s
  \end{lstlisting}
  \vfill
  The resulting {\tt .s} file contains the following code:
  \begin{lstlisting}
call    GOMP_parallel_start
leaq    80(%rsp), %rdi
call    setup._loopfn.0
call    GOMP_parallel_end
  \end{lstlisting}
  \vfill
  {\bf Note:} {\tt gcc} also parallelizes {\tt main.\_loopfn.2} and
  {\tt main.\_loopfn.3}, although it looks like it serves little purpose.
\end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Manually Parallelizing the Example Code}

\begin{changemargin}{1.5cm}
  What can we do to parallelize this code?
  \vfill
  {\bf Option 1:} \uncover<2->{ horizontal~ \qquad \begin{minipage}{7em} --- --- --- ---\\[-.8em] --- --- --- ---\\[-.8em] --- --- --- --- \end{minipage}}

  \begin{itemize}
    \item<2-> Create 4 threads; each thread does 1000 iterations on its own sub-array.
  \end{itemize}

  {\bf Option 2:} \uncover<3->{ bad horizontal~~ \begin{minipage}{7em} --- --- --- ---\\[-.8em] --- --- --- ---\\[-.8em] --- --- --- --- \end{minipage}}

  \begin{itemize}
    \item<3-> 1000 times, create 4 threads which each operate once on the sub-array.
  \end{itemize}

  {\bf Option 3:} \uncover<4->{~~vertical $ \quad \: \qquad \mid \mid \mid\mid \:\: \mid \mid \mid \mid \:\: \mid \mid \mid \mid\:\: \mid \mid \mid \mid$}
  \begin{itemize}
    \item<4-> Create 4 threads; for each element, the owning thread does 1000 iterations on that element.
  \end{itemize}
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Manual Parallelization Demo}

\begin{changemargin}{1.5cm}
  I'll show a demo of three example PThread parallelizations.\\[1em]

  Methodology: compiling with {\tt solarisstudio}, \\ flags {\tt -O3 -lpthread}.\\[1em]

  Which manual option performs better?
\end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Comparing Parallelization Results}

\begin{changemargin}{1.5cm}
How does autoparallelization compare to manual parallelization?
  %% \begin{itemize}
  %%   \item<2-> Relative ordering: {\bf Option 3} \textgreater{} Automatic
  %%     \textgreater{} {\bf Option 1}
  %%   \item<2-> Automatic parallelization of {\bf Option 1} was better than
  %%     manual, why?
  %%   \item<3-> Manual {\bf Option 3} performed better, even though both used the
  %%     same number of threads, why?
  %% \end{itemize}
\end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Case Study 2: Multiplying a Matrix by a Vector}

\begin{changemargin}{1.5cm}
Let's see how automatic parallelization does on a more complicated
program (could we parallelize this?):

  \begin{lstlisting}[numbers=left]
void matVec (double **mat, double *vec, double *out,
             int *row, int *col) 
{
  int i, j;
  for (i = 0; i < *row; i++)
  {
    out[i] = 0;
    for (j = 0; j < *col; j++)
    {
      out[i] += mat[i][j] * vec[j];
    }
  }
}
  \end{lstlisting}
\end{changemargin}

  \begin{center}
    Reminder:
    \begin{math}
      \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6\end{bmatrix}
      \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
      =
      \begin{bmatrix} 14 \\ 32 \end{bmatrix}
    \end{math}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Case Study: Automatic Parallelization, Attempt 1}

  \begin{changemargin}{2.5cm}
  Well, based on our knowledge, we could parallelize the outer loop.\\[1em]
  Let's see what {\tt solarisstudio} will do for us\ldots
  \end{changemargin}

  \begin{lstlisting}
% solarisstudio-cc -xautopar -xloopinfo -O3 -c fploop.c
"fploop.c", line 5: not parallelized, not a recognized for loop
"fploop.c", line 8: not parallelized, not a recognized for loop
  \end{lstlisting}

  \begin{changemargin}{2.5cm}
  \ldots it refuses to do anything, guesses?
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Case Study: Automatic Parallelization, Attempt 2}

\begin{changemargin}{1cm}
  \begin{itemize}
    \item The loop bounds are not constant, since one of the variables may alias
      {\tt row} or {\tt col}, even though {\tt int $\neq$ double}.
  \end{itemize}
~\\[1em]

  So, let's add {\tt restrict} to {\tt row} and {\tt col} and see what
  happens\ldots

  \begin{lstlisting}
% solarisstudio-cc -O3 -xautopar -xloopinfo -c fploop.c
"fploop.c", line 5: not parallelized, unsafe dependence
"fploop.c", line 8: not parallelized, unsafe dependence
  \end{lstlisting}

  Now it recognizes the loop, but still won't parallelize it. Why?
\end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Case Study: Automatic Parallelization, Attempt 3}

\begin{changemargin}{1cm}
  \begin{itemize}
    \item {\tt out} might alias {\tt mat} or {\tt vec}, which would make this
      unsafe
  \end{itemize}

  Let's add another {\tt restrict} to {\tt out}:

  \begin{lstlisting}
% solarisstudio-cc -O3 -xautopar -xloopinfo -c fploop.c
"fploop.c", line 5: PARALLELIZED, and serial version
  generated
"fploop.c", line 8: not parallelized, unsafe dependence
  \end{lstlisting}

  Now, we can get the outer loop to parallelize.
  
  \begin{itemize}
    \item Parallelizing the outer loop is almost always better than inner loops,
      and usually it's a waste to do both, so we're done.
  \end{itemize}

  {\bf Note:} We can parallelize the inner loop as well (it's similar to
  Assignment 1). We'll see that {\tt solarisstudio} can do it automatically.
\end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Loops That gcc's Automatic Parallelization Can Handle}

\begin{changemargin}{2.5cm}
  Single loop:
  \begin{lstlisting}
for (i = 0; i < 1000; i++)
    x[i] = i + 3;
  \end{lstlisting}
  \vfill
  Nested loops with simple dependency:
  \begin{lstlisting}
for (i = 0; i < 100; i++)
    for (j = 0; j < 100; j++)
        X[i][j] = X[i][j] + Y[i-1][j];
  \end{lstlisting}
  \vfill
  Single loop with not-very-simple dependency:
  \begin{lstlisting}
for (i = 0; i < 10; i++)
    X[2*i+1] = X[2*i];
  \end{lstlisting}
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Loops That gcc's Automatic Parallelization Can't Handle}

\begin{changemargin}{2.5cm}
  Single loop with if statement:
  \begin{lstlisting}
for (j = 0; j <= 10; j++)
    if (j > 5) X[i] = i + 3;
  \end{lstlisting}
  \vfill
  Triangle loop:
  \begin{lstlisting}
for (i = 0; i < 100; i++)
    for (j = i; j < 100; j++)
        X[i][j] = 5;
  \end{lstlisting}
  \vfill
  Examples from: \url{http://gcc.gnu.org/wiki/AutoparRelated}
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Summary of Conditions for Automatic Parallelization}

\begin{changemargin}{2.5cm}
  From Chapter 10 of
  Oracle's \emph{Fortran Programming Guide}
  \footnote{\scriptsize \url{http://download.oracle.com/docs/cd/E19205-01/819-5262/index.html}}
  translated to C, a loop must:

  \begin{itemize}
    \item have a recognized loop style, e.g., {\tt for} loops with
      bounds that don't vary per-iteration;
    \item have no dependencies between data accessed in loop bodies for
      each iteration;
    \item not conditionally change scalar variables read after the loop
      terminates, or change any scalar variable across iterations; and
    \item have enough work in the loop body to make parallelization profitable.
  \end{itemize}
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Reductions}

\begin{changemargin}{2.5cm}
  \begin{itemize}
    \item Reductions combine input data into a smaller (summary) set.
    \item We'll see a more complete definition when we touch on functional
      programming.
    \item Simplest instance: computing the sum of an array.
  \end{itemize}
~\\

  Consider the following code:

  \begin{lstlisting}
double sum (double *array, int length)
{
  double total = 0;

  for (int i = 0; i < length; i++)
    total += array[i];
  return total;
}
  \end{lstlisting}

  Can we parallelize this? 
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Reduction Problems}

\begin{changemargin}{1cm}
  Barriers to parallelization:
  \begin{enumerate}
    \item value of {\tt total} depends on previous
      iterations;
    \item addition is actually non-associative for floating-point values
      \\ (is this a problem?)
  \end{enumerate}
  ~\\[1em]
  \begin{center}
    Recall that ``associative'' means: 
     \[a + (b + c) = (a + b) + c.\]
  \end{center}
    \only<2-> In this case, the program probably isn't sensitive to rounding,
      but you should always consider if an operation is associative.
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Automatic Parallelization via Reduction}

\begin{changemargin}{1.5cm}
  If we compile the program with {\tt solarisstudio} and add the flag
  {\tt -xreduction}, it will parallelize the code:
\end{changemargin}

  \begin{lstlisting}
% solarisstudio-cc -xautopar -xloopinfo -xreduction -O3 -c sum.c 
"sum.c", line 5: PARALLELIZED, reduction, and serial version
  generated
  \end{lstlisting}
~\\[1em]

\begin{changemargin}{1.5cm}
  {\bf Note:} If we try to do the reduction on {\tt fploop.c} with {\tt restrict}s added, we'll get the following:
\end{changemargin}

  \begin{lstlisting}
% solarisstudio-cc -O3 -xautopar -xloopinfo  -xreduction -c fploop.c
"fploop.c", line 5: PARALLELIZED, and serial version generated
"fploop.c", line 8: not parallelized, not profitable
  \end{lstlisting}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Dealing with Function Calls}

\begin{changemargin}{1.5cm}  
  \begin{itemize}
    \item A general function could have arbitary side effects.
    \item Production compilers tend to avoid parallelizing any loops with
      function calls.
  \end{itemize}
~\\[1em]

  Some built-in functions, like {\tt sin()}, are ``pure'', have no side
  effects, and are safe to parallelize.
~\\[1em]
  {\bf Note:} this is why functional languages are nice for parallel
  programming: impurity is visible in type signatures.
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Dealing with Function Calls in {\tt solarisstudio}}

\begin{changemargin}{1.5cm}  
  \begin{itemize}
    \item For {\tt solarisstudio} you can use the {\tt -xbuiltin} flag to make
      the compiler use its whitelist of ``pure'' functions.
    \item The compiler can then parallelize a loop which uses {\tt sin()}
      (you shouldn't replace built-in functions with your own if you use this
      option).
  \end{itemize}
  \vfill
  Other options which may work:
  
  \begin{enumerate}
    \item Crank up the optimization level ({\tt -xO4}).
    \item Explicitly tell the compiler to inline certain functions ({\tt
      -xinline=}, or use the {\tt inline} keyword).
  \end{enumerate}
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Summary of Automatic Parallelization}

\begin{changemargin}{1.5cm}  
To help the compiler, we can:
\begin{itemize}
\item use {\tt restrict} (make a {\tt restrict}ed copy); and,
\item make sure that loop bounds are constant (temporary
variables). 
\end{itemize}
~\\
Some compilers automatically create different versions
for the alias-free case and the (parallelized) aliased case.\\[1em]

At runtime, the program runs the aliased case if correct.
\end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
