\documentclass[aspectratio=43]{beamer}

% Text packages to stop warnings
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{multirow}
\usepackage{tikz}

\usetikzlibrary{arrows,automata,shapes,positioning}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=2.5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{bw} = [rectangle, draw, fill=blue!20, 
    text width=3.5em, text centered, rounded corners, minimum height=2em]

% Themes
\usetheme{Boadilla}
\setbeamertemplate{footline}[page number]{}
\setbeamertemplate{navigation symbols}{}

% Suppress the navigation bar
\beamertemplatenavigationsymbolsempty

\lstset{basicstyle=\scriptsize, frame=single}

\newenvironment{changemargin}[1]{% 
  \begin{list}{}{% 
    \setlength{\topsep}{0pt}% 
    \setlength{\leftmargin}{#1}% 
    \setlength{\rightmargin}{1em}
    \setlength{\listparindent}{\parindent}% 
    \setlength{\itemindent}{\parindent}% 
    \setlength{\parsep}{\parskip}% 
  }% 
  \item[]}{\end{list}} 

\title{Lecture 18---Automatic Parallelization, OpenMP}
\subtitle{ECE 459: Programming for Performance}
\date{February 13, 2015}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[plain]
  \titlepage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Road Map}

  \begin{changemargin}{1cm}
    \begin{itemize}
    \item Previously: compilers \& automatic parallelization, OpenMP.
    \item Now: More OpenMP.
    \item Soon: Reading week.
    \end{itemize}
  \end{changemargin}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Reductions}
  \begin{changemargin}{2cm}
Recall that we introduced the concept of a reduction, e.g.
\begin{lstlisting}[language=C]
for (int i = 0; i < length; i++)
    total += array[i];
\end{lstlisting}

What is the appropriate scope for {\tt total}? \\
\pause
Well, it should be
\structure{shared}.

\begin{itemize}
 \item We want each thread to be able to write to it. \pause
 \item But, is there a race condition? (of course)
\end{itemize}

Aha! OpenMP can deal with reductions as a special case:


\begin{lstlisting}
#pragma omp parallel for reduction (+:total)
\end{lstlisting}

specifies that the {\tt total} variable is the accumulator for a
reduction over the {\tt +} operator.
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Accessing Private Data outside a Parallel Region}

  \begin{changemargin}{2cm}
Sometimes you want \structure{private} variables, but want them initialized
before the loop.\\[1em]

Consider this (silly) code:

\begin{lstlisting}[language=C]
int data=1;
#pragma omp parallel for private(data)
for (int i = 0; i < 100; i++)
    printf ("data=%d\n", data);
\end{lstlisting}


\begin{itemize}
  \item {\tt data} is private, so OpenMP will not copy in initial 1.
  \item To make OpenMP copy the data before the threads start, use
    {\tt firstprivate(data)}.
  \item To publish a variable after the (sequentially) last iteration of the loop, use
    {\tt lastprivate(data)}.
\end{itemize}
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Thread-Private Data}

  \begin{changemargin}{1.5cm}

    You might have a global variable, for which each thread should have a persistent local copy---lives across parallel regions.\\[1em]
  \begin{itemize}
    \item Use the {\tt threadprivate} directive.
    \item Add {\tt copyin} if you want something like
      {\tt firstprivate}.
    \item There is no {\tt lastprivate} since the data is accessible after the
      loop.
  \end{itemize}
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Thread-Private Data Example (1)}

  \begin{changemargin}{1cm}
  \begin{lstlisting}
#include <omp.h>
#include <stdio.h>

int tid, a, b;

#pragma omp threadprivate(a)

int main(int argc, char *argv[])
{
    printf("Parallel #1 Start\n");
    #pragma omp parallel private(b, tid)
    {
        tid = omp_get_thread_num();
        a = tid;
        b = tid;
        printf("T%d: a=%d, b=%d\n", tid, a, b);
    }

    printf("Sequential code\n");
  \end{lstlisting}
  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Thread-Private Data Example (2)}

  \begin{changemargin}{1cm}
  \begin{lstlisting}
    printf("Parallel #2 Start\n");
    #pragma omp parallel private(tid)
    {
        tid = omp_get_thread_num();
        printf("T%d: a=%d, b=%d\n", tid, a, b);
    }

    return 0;
}    
  \end{lstlisting}
  \end{changemargin}
  
  \begin{center}
    \begin{columns}[c]
      \column{1.5in}
        \begin{lstlisting}
% ./a.out
Parallel #1 Start
T6: a=6, b=6
T1: a=1, b=1
T0: a=0, b=0
T4: a=4, b=4
T2: a=2, b=2
T3: a=3, b=3
T5: a=5, b=5
T7: a=7, b=7
        \end{lstlisting}
      \column{1.5in}
        \begin{lstlisting}
Sequential code
Parallel #2 Start
T0: a=0, b=0
T6: a=6, b=0
T1: a=1, b=0
T2: a=2, b=0
T5: a=5, b=0
T7: a=7, b=0
T3: a=3, b=0
T4: a=4, b=0
        \end{lstlisting}
    \end{columns}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Collapsing Loops}


  \begin{itemize}
    \item Normally, it's best to parallelize the outermost loop.
  \end{itemize}

  \begin{changemargin}{1.5cm}

  ~\\
  Consider this code:

  \begin{lstlisting}[language=C]
#include <math.h>
int main() {
    double array[2][10000];
    #pragma omp parallel for collapse(2)
    for (int i = 0; i < 2; i++)
      for (int j = 0; j < 10000; j++)
        array[i][j] = sin(i+j);
    return 0;
}
  \end{lstlisting}


  \begin{itemize}
  \item Would parallelizing this outer loop benefit us?\\
    What about the inner loop?
  \end{itemize}

  OpenMP supports \emph{collapsing} loops:

  \begin{itemize}
    \item Creates a single loop for all the iterations of the two loops.
    \item Outer loop only enables the use of 2 threads.
    \item Collapsed loop lets us use up to 20,000 threads.
  \end{itemize}
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Better Performance Through Scheduling: An Example}

  \begin{changemargin}{1cm}

  Default mode: \emph{Static scheduling}.
  \begin{itemize}
    \item Assumes each iteration takes the same running time.
  \end{itemize}

  Does that assumption hold for this code?

  \begin{lstlisting}[language=C]
double calc(int count) {
    double d = 1.0;
    for (int i = 0; i < count*count; i++) d += d;
    return d;
}

int main() {
    double data[200][100];
    int i, j;
    #pragma omp parallel for private(i, j) shared(data)
    for (int i = 0; i < 200; i++) {
        for (int j = 0; j < 200; j++) {
            data[i][j] = calc(i+j);
        }
    }
    return 0;
}
  \end{lstlisting}
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Better Performance Through Scheduling}

\large
  \begin{changemargin}{.7cm}
  
  \begin{itemize}
    \item In example, earlier iterations are faster than later iterations.\\
          Result: sublinear scaling---wait for all iterations to finish.\\[1em]
    \item Turn on \emph{dynamic schedule} mode\\ by adding
      {\tt schedule(dynamic)} to the pragma:
      \begin{itemize}
        \item Breaks the work into chunks;
        \item Distributes the work to each thread in chunks;
        \item Higher overhead;
        \item Default chunk size of 1\\ (can modify, e.g. {\tt schedule(dynamic, n/50)}).
      \end{itemize}
  \end{itemize}
  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{More Scheduling}

  \begin{changemargin}{1cm}
  
  Other schedule modes exist, e.g. {\tt guided}, {\tt auto} and {\tt runtime}.
      
  \begin{itemize}
    \item {\tt guided} changes the chunk size based on work
          remaining.
          
      \begin{itemize}
        \item Default minimum chunk size = 1 (can modify)
      \end{itemize}
    \item {\tt auto} lets OpenMP decide what's best.
    \item {\tt runtime} doesn't pick a mode until runtime.
      \begin{itemize}
        \item Tune with \verb+OMP_SCHEDULE+ environment variable
      \end{itemize}
  \end{itemize}
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Beyond {\tt for} loops: OpenMP~Parallel~Sections~and~Tasks}
\frame{\partpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Why more than for?}

\large
  \begin{changemargin}{1cm}
  So far, we can parallelize (some) {\tt for} loops with OpenMP.\\[1em]

  Less powerful than Pthreads. (Also harder to get wrong.)\\[1em]

  Reflects OpenMP's scientific-computation heritage.\\[1em]

  Today, we need more general parallelism, not just matrices.
  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Parallel Sections Example: Linked Lists (1)}

\large
  \begin{changemargin}{2cm}
  Purely-static mechanism for specifying independent work units which should run in
  parallel.\\[1em]

  Linked list example:

{\small
\begin{lstlisting}[language=C]
  #include <stdlib.h>

  typedef struct s { struct s* next; } S;

  void setuplist (S* current) {
    for (int i = 0; i < 10000; i++) {
      current->next = (S*) malloc (sizeof(S));
      current = current->next;
    }
    current->next = NULL;
  }
  \end{lstlisting}
}

  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Parallel Sections Example: Linked Lists (2)}
  \begin{changemargin}{2cm}

  (Exactly) 2 linked lists:
{\small
  \begin{lstlisting}
  int main() {
    S var1, var2;
    #pragma omp parallel sections
    {
      #pragma omp section
      { setuplist (&var1); }
      #pragma omp section
      { setuplist (&var2); }
    }
    return 0;
  }
\end{lstlisting}}

  Parallelism structure explicitly visible.\\[1em]
  Finite number of threads.\\[1em]
  (What's another barrier to parallelism here?)

  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Nested Parallelism}
  \begin{changemargin}{2cm}

  Sometimes you don't want to collapse loops.\\[1em]

  Example: (better example in PDF notes!)\\[1em]

{\small
  \begin{lstlisting}
    #pragma omp parallel sections
    {
      #pragma omp section
      { 
          #pragma omp parallel for
          for (int i = 0; i < 1000; i++) { ... } 
      }
      #pragma omp section
      {
          #pragma omp parallel for
          for (int i = 0; i < 1000; i++) { ... } 
      }
    }
\end{lstlisting}}

  To enable nested parallelism, call \verb+omp_set_nested(1)+ or
  set \verb+OMP_NESTED+. (Runtime might refuse.)\\[1em]

  
  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{OpenMP Tasks}
  \begin{changemargin}{2cm}

  Main new feature in OpenMP 3.0.\\[1em]

  \verb+#pragma omp task+:\\ \qquad code splits off and scheduled to run later.\\[1em]

  More flexible than parallel sections: 
  \begin{itemize}
   \item can run as many threads as needed;
   \item tasks do not need to join (like detached threads).
  \end{itemize}
  OpenMP does the task-to-thread mapping---lower overhead.

  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Examples of tasks}
  \begin{changemargin}{1cm}

\Large
   Two examples:
   \begin{itemize}
     \item web server\\ \quad unstructured requests
     \item user interface\\ \quad allows users to start concurrent tasks
   \end{itemize}

  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Boa webserver main loop example}
  \begin{changemargin}{2cm}

{\small
\begin{lstlisting}[language=C,morekeywords={foreach,pragma,omp,parallel,single,nowait,task,untied}]
#pragma omp parallel
  /* a single thread manages the connections */
  #pragma omp single nowait
  while (!end) {
    process any signals
    foreach request from the blocked queue {
      if (request dependencies are met) {
        extract from the blocked queue
        /* create a task for the request */
        #pragma omp task untied
          serve_request(request);
      }
    }
    if (new connection) {
      accept_connection();
      /* create a task for the request */
      #pragma omp task untied
        serve_request(new connection);
    }
    select();
  }
\end{lstlisting}
}

  \end{changemargin}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Other OpenMP qualifiers}

  \begin{changemargin}{2cm}
    {\tt untied}: lifts restrictions on task-to-thread mapping.\\[1em]

    {\tt single}: only one thread runs the next statement (not $N$ copies).\\[1em]

    {\tt flush} directive: write all values in registers or cache to memory.\\[1em]

    {\tt barrier}: wait for all threads to complete. (OpenMP also has implicit
barriers at ends of parallel sections.)\\[1em]

    OpenMP also supports critical sections (one thread at a time), atomic
 sections, and typical mutex locks (\verb+omp_set_lock+,
 \verb+omp_unset_lock+).
  \end{changemargin}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
